# -*- coding: utf-8 -*-
"""AccidRoutier-RF_XGBOOST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dkV8z0BirW1OgGouDQcFmoIyl_b4A5V_
"""

# 1) Import des packages nécessaires
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from sklearn.preprocessing import normalize
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

# Load data
df = pd.read_csv('/content/drive/MyDrive/fusion3.csv', sep =',', low_memory=False)
df.info()
df.dtypes
df.head()

#3)
df.info()

# 4)
df.dtypes

#5)
## Calcul des correlations et des variances

cor = df.corr()
fig, ax = plt.subplots(figsize =(12,12))
sns.heatmap(cor, annot=True)

## les valeurs des corrélations sont loin de 1 ou -1 : Pas exploitation à ce niveau

#6) Calcul des variances
var = df.var()
var

## Calcul des variances des variables categorielles ##

from collections import Counter
categ_columns = df.select_dtypes(include=['object']).columns
df_categ = df[categ_columns]
df_categ

from collections import Counter
categ_columns = df.select_dtypes(include=['object']).columns
df_categ = df[categ_columns]

categ_counts = Counter(df_categ)
total_samples = len(df_categ)

categ_probabilities = [count / total_samples for count in categ_counts.values()]
mean_probability = sum(categ_probabilities) / len(categ_probabilities)

variance_categ = sum((p - mean_probability)**2 for p in categ_probabilities) / len(categ_probabilities)

print("Variance des valeurs catégorielles :", variance_categ)

## Aucun résultat satisfaisont avec les calculs des variances

#7) SELECTION DES VARIABLES FEATURES

# Cration de nouvelle datset pour appliquer les modification des features selectionnées

accid = df.copy(all)

XY= df.copy(all)

XY.info()

y = XY['grav']
y

y

X = XY.drop('grav', axis=1, inplace=True)

X

XY

# variables explicatives catégorielles
Xc = X.drop(['Unnamed: 0', 'num_acc','place', 'an_nais', 'an_naiss', 'age_acc_an', 'place', 'nbv', 'occutc'], axis=1, inplace=True)

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)

X_train_he_sex = pd.get_dummies(X_train['sexe'])

X_train_he_catu = pd.get_dummies(X_train['catu'])

#8) Encoder des variables catégorielles (et memes les numériques)
# Encodage get_dummies des variables catégorielles
for col in X.select_dtypes("category"):
    X = pd.get_dummies(X, columns=[col])

# Affichez la base de données encodées
print(X.head())

from sklearn.preprocessing import OneHotEncoder
# Créez un objet OneHotEncoder()
encoder = OneHotEncoder()
# Encodage one-hot des variables catégorielles
for col in X.select_dtypes("category"):
    X[col] = encoder.fit_transform(X[col].values.reshape(-1, 1))

# Affichez la base de données encodées
print(X.head())

# Créez un objet OneHotEncoder()
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()

# Encodage one-hot des variables catégorielles
for col in X.select_dtypes("category"):
    X[col] = encoder.fit_transform(df[col].values.reshape(-1, 1))

# Enregistrez le jeu de données encodé
X.to_csv("X_encoded.csv")



X_train_he_heure = pd.get_dummies(X_train['heure'])

X_train_he_heure

X_train_he_catu

X_train_he_sex

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
model = LogisticRegression()
cross_val = KFold(n_splits=5, random_state=42)
scores = cross_val_score(model, x_train, y_train, cv=cross_val, scoring='roc_auc')
print("Mean AUC Score - Logistic Regression: ", scores.mean())
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_trains = scaler(X_train)

from sklearn.metrics import roc_auc_score

micro_roc_auc_ovr = roc_auc_score(
    y_test,
    y_score,
    multi_class="ovr",
    average="micro",
)

print(f"Micro-averaged One-vs-Rest ROC AUC score:\n{micro_roc_auc_ovr:.2f}")

# 11) Initialise le modèle RandomForest et contruit:

model_rf = RandomForestClassifier(n_estimators=100, max_depth=8)

# 12) Entrainement du modèle RandomForest :

model_rf.fit(X_train, y)

X_tran = pd.get_dummies(X_train)













## Selection des features
## Pour les variables non catégorielles à reorganiser :
# heure ( categoriser les horaires - serie pandas chaque horaire est associée à la catégorie qui lui correspond)
heure_label=pd.cut(accid['heure'],24,labels=[str(i) for i in range(0,24)])
# Remplace la colonne des horaires des accidents par la colonne des catégories d’horaires
accid['heure']=heure_label.values
print(accid['heure'])

## Variables numériques rejetées : num_acc, place (plan tracé), age_acc_an (age au moment de l'accident),
## nbv (nombre total de voirie de circulation), occutc(Nombre d'Occupants dans le transport commun)

# Categorize the 'heure' column into 24 bins and assign labels
heure_label = pd.cut(accid['heure'], bins=24, labels=[str(i) for i in range(24)])
# Update the 'heure' column with the labels

heure_label

accid['heure'] = heure_label.values
accid['heure']

## Selection des features catégorielles
features_num = ['num_acc','place','age_acc_an','nbv','occutc','heure']
features_cat = ['heure','catu','sexe','trajet','locp',
           'actp','etatp','an_nais','num_veh','secuUn','secuDeux','an_naiss','tranches_ages','catr','circ',
           'vosp','prof','plan','surf','infra','situ','senc','obs','obsm','choc','manv','catv_Label',
            'permis','lum','agg','int','atm', 'col','com','dep','date','jour_de_la_semaine']
features= ['num_acc','place','age_acc_an','nbv','occutc','heure','catu','sexe','trajet','locp',
           'actp','etatp','an_nais','num_veh','secuUn','secuDeux','an_naiss','tranches_ages','catr','circ',
           'vosp','prof','plan','surf','infra','situ','senc','obs','obsm','choc','manv','catv_Label',
            'permis','lum','agg','int','atm', 'col','com','dep','date','jour_de_la_semaine']
features_short= ['num_acc','place','heure','date','jour_de_la_semaine']

#7
y = accid['grav']
y

for col in accid.columns:
  print(col)

X = accid
X.head()

#8) Encoder des variables catégorielles (et memes les numériques)
X = pd.get_dummies(X.astype(str))

#8) Encoder des variables catégorielles (et memes les numériques)
X_train_accid = pd.get_dummies(accid[features].astype(str))

X_train_accid_cat.info()

#9) On commence par normaliser les données avec X_trained_cat :

X_train_accid_cat = normalize(X_train_accid_cat.values)

#   (A REVOIR SANS EXECUTER) 9) On commence par normaliser les données

X_train_accid = normalize(X_train_accid.values)

# 10) On divise la base en bases d'entraînements et de test :

X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_train_accid_cat,y)

# 11) Initialise le modèle RandomForest et contruit:

model_rf = RandomForestClassifier(n_estimators=100, max_depth=8)

# 12) Entrainement du modèle RandomForest :

model_rf.fit(X_train_rf, y_train_rf)

#13) Prédictions de la gravité des accidents sur les données d'entrainement

predict_train = model_rf.predict(X_train_rf)

# 14) Prédictions de la gravité des accidents sur les données de test

predict_test = model_rf.predict(X_test_rf)

# 15) Calcul et Affichage des scores des prédictions :

train_acc_rf = accuracy_score(y_train_rf, predict_train)
print('Score Prédiction des entrainements :',train_acc_rf)

test_acc_rf = accuracy_score(y_test_rf, predict_test)
print('Score Prédiction du test :',test_acc_rf)

#16) Construction du modèle XGBOOST
# Redécoupage du jeu de données fusion3.csv

X_train, X_test, y_train, y_test = train_test_split(X_train_accid_cat,y)

# 17) Création du modèle Gradient Boosting Classificateur :
model_boosting = GradientBoostingClassifier(loss="deviance",
    learning_rate=0.2,
    max_depth=5,
    max_features="sqrt",
    subsample=0.95,
    n_estimators=200)

#18) Entraînement du modèle sur le jeu d'entrainement :

model_boosting.fit(X_train, y_train)

# 19)   Prédictions avec le modèle XGBOOST
predict_test_xgb = model_boosting.predict(X_test)
predict_train_xgb = model_boosting.predict(X_train)

# Affichage des résultats :

train_acc_xgb = accuracy_score(y_train, predict_train_xgb)
print('Score Prédiction XGBOOST des entrainements :',train_acc_xgb)

test_acc_xgb = accuracy_score(y_test, predict_test_xgb)
print('Score Prédiction XGBOOST du Test :',test_acc_xgb)

# 19)   Les resultats avec RandomForest et XGBOOST :
print('Score Prédiction RandomForest du test  :',test_acc_rf)
print('Score Prédiction XGBOOST du Test :',test_acc_xgb)

